{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33f1bcce",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20f3186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df= pd.read_csv('results.csv')\n",
    "wc_games = pd.read_excel('wc_results.xlsx')\n",
    "N = len(wc_games)\n",
    "df = pd.concat([df, wc_games], axis=0)\n",
    "df['date'] = pd.to_datetime(df['date'], dayfirst=True)\n",
    "\n",
    "# Only include data from 1995 onwards. This marks roughly the beginning of the professional era\n",
    "df = df[df['date'] > '1995-01-01']\n",
    "\n",
    "def determine_winner_and_loser(row):\n",
    "    # Function which adds winner and loser columns to the df\n",
    "    if row['home_score'] > row['away_score']:\n",
    "        winner = row['home_team']\n",
    "        loser = row['away_team']\n",
    "    elif row['home_score'] < row['away_score']:\n",
    "        winner = row['away_team']\n",
    "        loser = row['home_team']\n",
    "    else:\n",
    "        winner = 'Draw'\n",
    "        loser = 'Draw'\n",
    "    return winner, loser\n",
    "\n",
    "def calculate_team_form(df, team_name, current_row_index, n_games):\n",
    "    # Function which calculates form of each team over the last N games\n",
    "    team_form = []\n",
    "    count = 0\n",
    "    games = df.copy()\n",
    "    games_b = games.iloc[:current_row_index]\n",
    "    \n",
    "    for index in range(len(games_b)-1, -1, -1):\n",
    "        row1 = games_b.iloc[index]\n",
    "\n",
    "        if row1['home_team'] == team_name or row1['away_team'] == team_name:\n",
    "\n",
    "            if row1['winner'] == 'draw':\n",
    "                team_form.append(0.5)\n",
    "            elif row1['winner'] == team_name:\n",
    "                team_form.append(1)\n",
    "            else:\n",
    "                team_form.append(0)\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "    return sum(team_form[:n_games])\n",
    "\n",
    "# Apply the function to create new 'winner' and 'loser' columns\n",
    "df[['winner', 'loser']] = df.apply(determine_winner_and_loser, axis=1).apply(pd.Series)\n",
    "\n",
    "# Initialise columns to store the live ranking points of both teams\n",
    "df['ranking_points_home'] = 0\n",
    "df['ranking_points_away'] = 0\n",
    "\n",
    "# Add a column representing the margin in favour of the 'home' team\n",
    "df['margin'] = df['home_score'] - df['away_score']\n",
    "\n",
    "# Add a column which specifies match result as either home_win away_win or draw\n",
    "df['result'] = df['margin'].apply(lambda x: 'home_win' if x > 0 else ('away_win' if x < 0 else 'draw'))\n",
    "\n",
    "# Initialise rankings dictionary\n",
    "ranking_points = {'Scotland': 80, 'England': 80, 'Wales': 80, 'Italy': 80, 'France': 80, 'Ireland': 80, 'New Zealand': 80, 'Argentina': 80, 'South Africa': 80, 'Australia': 80 }\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # For each match in the dataframe add the live rankings of both teams which are kept track of in the dictionary ranking_points\n",
    "    home_team = row['home_team']\n",
    "    away_team = row['away_team']\n",
    "    \n",
    "    # Update ranking_points_home and ranking_points_away\n",
    "    df.at[i, 'ranking_points_home'] = ranking_points[home_team]\n",
    "    df.at[i, 'ranking_points_away'] = ranking_points[away_team]\n",
    "    if row['neutral'] == True:\n",
    "        home_points = ranking_points[home_team]\n",
    "    else:\n",
    "        home_points = ranking_points[home_team] + 3\n",
    "    away_points = ranking_points[away_team]\n",
    "    gap = home_points - away_points\n",
    "    if gap < -10:\n",
    "        gap = -10\n",
    "    elif gap > 10:\n",
    "        gap = 10\n",
    "    if row['winner'] == 'Draw':\n",
    "        core = gap*0.1\n",
    "    elif row['winner'] == home_team:\n",
    "        core = 1 - (gap*0.1)\n",
    "    else:\n",
    "        core = 1 + (gap*0.1)\n",
    "        \n",
    "    if np.abs(row['home_score'] - row['away_score']) > 15:\n",
    "        core *= 1.5\n",
    "        \n",
    "    if row['world_cup'] == True:\n",
    "        core *= 2\n",
    "        \n",
    "    if row['winner'] != 'Draw':\n",
    "        ranking_points[row['winner']] += core\n",
    "        ranking_points[row['loser']] -= core\n",
    "    else:\n",
    "        ranking_points[home_team] -= core\n",
    "        ranking_points[away_team] += core\n",
    "        \n",
    "    \n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    df.at[idx, 'home_form'] = calculate_team_form(df, row['home_team'], idx, n_games=5)\n",
    "    df.at[idx, 'away_form'] = calculate_team_form(df, row['away_team'], idx, n_games=5)\n",
    "    \n",
    "wc_games = df.iloc[-N:]\n",
    "df_sliced = df.iloc[:-N]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd6a68",
   "metadata": {},
   "source": [
    "## Predicting Points Margin between Team 1 and Team 2. \n",
    "\n",
    "A positive margin is in favour of team 1 (or the team designated as home in the dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "388e4e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Linear Regression\n",
      "Mean Squared Error (MSE): 203.76\n",
      "R-squared (R2): 0.45\n",
      "==================================================\n",
      "Model: Random Forest Regressor\n",
      "Mean Squared Error (MSE): 226.61\n",
      "R-squared (R2): 0.38\n",
      "==================================================\n",
      "Model: Support Vector Regressor\n",
      "Mean Squared Error (MSE): 234.31\n",
      "R-squared (R2): 0.36\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Consider games from 1996 onwards so the rankings have had time to calibrate\n",
    "data = df_sliced.copy()\n",
    "data = data[data['date'] > '1996-01-01']\n",
    "\n",
    "# Encode the neutral and world cup columns as binary indicator variables\n",
    "data['neutral'] = data['neutral'].astype(int)\n",
    "data['world_cup'] = data['world_cup'].astype(int)\n",
    "\n",
    "# Split into train and test datasets, using 1/1/2017 as the cut-off point. \n",
    "# Models are trained on data from games before this date and evaluated on data from games after this date\n",
    "train_data = data[data['date'] < '2017-01-01']\n",
    "test_data = data[data['date'] >= '2017-01-01']\n",
    "X_train = train_data[['neutral', 'world_cup', 'ranking_points_home', 'ranking_points_away', 'home_form', 'away_form']]\n",
    "y_train = train_data['margin']\n",
    "X_test = test_data[['neutral', 'world_cup', 'ranking_points_home', 'ranking_points_away', 'home_form', 'away_form']]\n",
    "y_test = test_data['margin']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Support Vector Regressor': SVR()\n",
    "}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate mean squared error (MSE) and R-squared (R2) for evaluation\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "    print(f\"R-squared (R2): {r2:.2f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de9597",
   "metadata": {},
   "source": [
    "## Predicting Match Winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e418bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results\n",
      "Accuracy: 0.68\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    away_win       0.63      0.60      0.61       121\n",
      "        draw       0.00      0.00      0.00         9\n",
      "    home_win       0.71      0.77      0.74       185\n",
      "\n",
      "    accuracy                           0.68       315\n",
      "   macro avg       0.45      0.45      0.45       315\n",
      "weighted avg       0.66      0.68      0.67       315\n",
      "\n",
      "Logistic Regression Results:\n",
      "Accuracy: 0.74\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    away_win       0.71      0.65      0.68       121\n",
      "        draw       0.00      0.00      0.00         9\n",
      "    home_win       0.75      0.83      0.79       185\n",
      "\n",
      "    accuracy                           0.74       315\n",
      "   macro avg       0.49      0.49      0.49       315\n",
      "weighted avg       0.71      0.74      0.72       315\n",
      "\n",
      "==================================================\n",
      "Decision Tree Results:\n",
      "Accuracy: 0.57\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    away_win       0.49      0.49      0.49       121\n",
      "        draw       0.00      0.00      0.00         9\n",
      "    home_win       0.64      0.66      0.65       185\n",
      "\n",
      "    accuracy                           0.57       315\n",
      "   macro avg       0.38      0.38      0.38       315\n",
      "weighted avg       0.56      0.57      0.57       315\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamiewelsh/opt/anaconda3/envs/diss/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamiewelsh/opt/anaconda3/envs/diss/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jamiewelsh/opt/anaconda3/envs/diss/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_train = train_data['result']\n",
    "y_test = test_data['result']\n",
    "\n",
    "# Initialise and fit a Random Forest Classifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results for Random Forest\n",
    "print('Random Forest Results')\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and fit a Logistic Regression classifier\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "classification_rep_logreg = classification_report(y_test, y_pred_logreg)\n",
    "\n",
    "# Print results for Logistic Regression\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_logreg:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep_logreg)\n",
    "print(\"=\"*50)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and fit a Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_decision_tree = decision_tree.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Decision Tree model\n",
    "accuracy_decision_tree = accuracy_score(y_test, y_pred_decision_tree)\n",
    "classification_rep_decision_tree = classification_report(y_test, y_pred_decision_tree)\n",
    "\n",
    "# Print results for Decision Tree\n",
    "print(\"Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_decision_tree:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep_decision_tree)\n",
    "print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c12e7",
   "metadata": {},
   "source": [
    "## RWC 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bcb7b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results:\n",
      "Accuracy: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    away_win       1.00      1.00      1.00         1\n",
      "    home_win       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00         3\n",
      "   macro avg       1.00      1.00      1.00         3\n",
      "weighted avg       1.00      1.00      1.00         3\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train on all data pre RWC 2023 and test on the RWC games\n",
    "X_train = data[['neutral', 'world_cup', 'ranking_points_home', 'ranking_points_away', 'home_form', 'away_form']]\n",
    "y_train = data['result']\n",
    "X_test = wc_games[['neutral', 'world_cup', 'ranking_points_home', 'ranking_points_away', 'home_form', 'away_form']]\n",
    "y_test = wc_games['result']\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform both training and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit a Logistic Regression classifier\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred_logreg = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "accuracy_logreg = accuracy_score(y_test, y_pred_logreg)\n",
    "classification_rep_logreg = classification_report(y_test, y_pred_logreg)\n",
    "\n",
    "# Print results for Logistic Regression\n",
    "print(\"Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_logreg:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep_logreg)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f354db22",
   "metadata": {},
   "source": [
    "So far, all of the matches have been predicted correctly, perhaps surprisingly as SA beat Scotland."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907263c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
